\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[hidelinks]{hyperref}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

% Code listing style
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10}
}


\begin{titlepage}
    \centering

    \vfill
     {\large \today}

      \vfill
       \vspace{-200pt}

    {\LARGE\bfseries
    Neural Machine Translation
    Comparative Study of Encoder - Decoder and Decoder Only Architectures
    for English - Dutch Translation
    \par}

    

    \vfill
    \vspace{-200pt}
    {\Large
    Submitted To\\
    \textbf{www.transperfect.com}
    \par}


    \vfill
    \vspace{-200pt}
    {\Large
    Mrityunjaya Tiwari\\[0.3cm]
    Department of Information Technology\\
    Indian Institute of Information Technology, Allahabad
    \par}

    \vfill

   

\end{titlepage}



% \newpage
% \date{\today}
% \vspace{100pt}

% \title{\textbf{Neural Machine Translation:\\Comparative Study of Encoder-Decoder and Decoder-Only Architectures\\for English-Dutch Translation}}
% \vspace{100pt}
% \author{Mrityunjaya Tiwari\\Department of Information Technology\\Indian Institute of Information Technology, Allahabad}
% \vspace{100pt}

% \author {Submitted To www.transperfect.com}

\begin{document}

% \maketitle

\newpage
\begin{abstract}
This project explores the application of neural machine translation (NMT) for English-to-Dutch translation by comparing two fundamentally different approaches: an encoder-decoder architecture (mBART-50) and a decoder-only model (GPT-2 with LoRA). We fine-tuned both models on a large parallel corpus of 50,000 sentence pairs and evaluated their performance on both specialized software documentation and general domain text. Our findings reveal that the encoder-decoder approach significantly outperforms the decoder-only model, achieving BLEU scores of 19.26 and 18.55 on software and general domains respectively, compared to 0.57 and 1.50 for GPT-2+LoRA. This work demonstrates the continued effectiveness of specialized translation architectures over general-purpose language models for sequence-to-sequence tasks.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

Machine translation has evolved dramatically over the past decade, transitioning from phrase-based statistical methods to sophisticated neural architectures. The challenge of translating between languages isn't just about finding word-for-word correspondences—it requires understanding context, grammatical structure, and cultural nuances. This becomes particularly interesting when we look at language pairs like English and Dutch, which share Germanic roots but have diverged in significant ways over centuries.

In this project, I set out to answer a fundamental question: How do different neural architectures approach the translation problem? Specifically, I wanted to compare models that were designed for translation (encoder-decoder models) with models that learn translation as a special case of language generation (decoder-only models). The results, as we'll see, tell us something important about how architecture influences capability.

\subsection{Motivation}

Why English to Dutch? Beyond being a practical translation pair, these languages present interesting challenges. They're similar enough that a model should theoretically learn patterns, but different enough that naive approaches fail. Dutch word order, compound words, and grammatical gender create situations where direct translation isn't possible—you need to truly understand the source sentence.

The choice of comparing mBART-50 with GPT-2 wasn't arbitrary either. These models represent two philosophical approaches to language: mBART says "let's build something specifically for translation," while GPT-2 says "let's build something general and teach it translation." Understanding which approach works better, and why, has implications beyond just this project.

\subsection{Research Questions}

\begin{itemize}
    \item How do encoder-decoder models compare to decoder-only models for translation quality?
    \item Does domain specialization (software vs. general text) affect relative performance?
    \item Can parameter-efficient fine-tuning (LoRA) make general language models competitive for translation?
    \item What do the metric differences tell us about the fundamental capabilities of each architecture?
\end{itemize}

\section{Background and Related Work}

\subsection{Evolution of Neural Machine Translation}

Neural machine translation really took off in 2014 when researchers showed that you could treat translation as a sequence-to-sequence problem. The idea was elegant: encode the source sentence into a fixed representation, then decode it into the target language. Early systems used recurrent neural networks, but they struggled with long sentences because information would get lost in the encoding process.

The transformer architecture, introduced in 2017, changed everything. By using self-attention mechanisms, transformers could directly model relationships between any two words in a sentence, regardless of distance. This solved the long-range dependency problem that had plagued earlier approaches.

\subsection{Encoder-Decoder vs. Decoder-Only Models}

\textbf{Encoder-Decoder Architecture:} Think of this as a two-stage process. The encoder reads and understands the source sentence, building a rich representation of its meaning. The decoder then generates the translation word by word, attending back to the source sentence at each step. This design mirrors how translation naturally works—you read and understand, then express in the target language.

mBART-50 is a multilingual version of BART (Bidirectional and Auto-Regressive Transformer) trained on text from 50 languages. Its encoder processes the input bidirectionally, meaning each word can see the entire context, while the decoder generates output autoregressively, one token at a time.

\textbf{Decoder-Only Architecture:} Models like GPT-2 take a different approach. There's no separate encoder—everything happens in one autoregressive process. You give it a prompt like "Translate to Dutch: Hello" and it continues the text, hopefully outputting the translation. This is more general but potentially less optimal for translation because the model wasn't specifically designed for this task.

\subsection{Parameter-Efficient Fine-Tuning (LoRA)}

Fine-tuning entire large models is expensive. LoRA (Low-Rank Adaptation) offers an elegant solution: instead of updating all parameters, it adds small, trainable matrices to specific layers. The key insight is that the weight updates during fine-tuning typically have low "rank" (they exist in a lower-dimensional space), so you can approximate them with much smaller matrices.

For a weight matrix $W$, LoRA adds a trainable update: $W' = W + \Delta W = W + BA$, where $B$ and $A$ are small matrices with $r$ rank. If $W$ is $d \times d$ and $r \ll d$, we train $2dr$ parameters instead of $d^2$. In practice, this can reduce trainable parameters by 99\% while maintaining performance.

\section{Methodology}

\subsection{Dataset Construction}

Building a good training dataset is crucial. I aggregated data from multiple high-quality parallel corpora:

\textbf{OPUS-100:} A collection of parallel texts from the OPUS project, covering diverse domains. This gave us breadth—translations from news, legal documents, movie subtitles, and more.

\textbf{OPUS Books:} Translations of literature provide rich, natural language examples. Books contain complex sentence structures and narrative flow that help models learn coherence.

\textbf{Tatoeba:} A community-created collection of translated sentences. While smaller, these examples are carefully validated.

\textbf{CCMatrix:} Large-scale corpus extracted from web crawls. I sampled this carefully to avoid overwhelming the dataset with potentially noisy data.

After loading all sources, I applied quality filtering:

\begin{lstlisting}
def is_valid(example):
    en_len = len(example['en'].split())
    nl_len = len(example['nl'].split())
    return 5 <= en_len <= 150 and 5 <= nl_len <= 150
\end{lstlisting}

This filter ensures sentences are neither too short (lacking context) nor too long (potentially containing multiple ideas). The final training corpus contained 50,000 diverse, high-quality translation pairs.

\subsection{Model Architectures}

\subsubsection{mBART-50 Setup}

I started with the pre-trained mBART-50 model, which already understands 50 languages. The tokenizer needs to be configured for English input and Dutch output:

\begin{lstlisting}
tokenizer_mbart = AutoTokenizer.from_pretrained(
    "facebook/mbart-large-50-many-to-many-mmt",
    src_lang="en_XX",
    tgt_lang="nl_XX"
)
\end{lstlisting}

The preprocessing function prepares examples in the format mBART expects:

\begin{lstlisting}
def preprocess_mbart(examples):
    inputs = tokenizer_mbart(
        examples['en'], 
        max_length=128, 
        truncation=True, 
        padding='max_length'
    )
    with tokenizer_mbart.as_target_tokenizer():
        labels = tokenizer_mbart(
            examples['nl'], 
            max_length=128, 
            truncation=True, 
            padding='max_length'
        )
    inputs["labels"] = labels["input_ids"]
    return inputs
\end{lstlisting}

The \texttt{as\_target\_tokenizer()} context manager is crucial—it tells the tokenizer to add the Dutch language tag and appropriate special tokens for the target side.

\subsubsection{GPT-2 with LoRA}

GPT-2 wasn't designed for translation, so we need to teach it through prompting. The format matters:

\begin{lstlisting}
prompt = f"Translate to Dutch: {english_text}\n\nTranslation: {dutch_text}<|endoftext|>"
\end{lstlisting}

This creates a clear pattern: instruction, then output. During inference, we provide just the instruction and let the model complete it.

The LoRA configuration targets the attention layers:

\begin{lstlisting}
lora_config = LoraConfig(
    r=16,                              # Rank
    lora_alpha=32,                     # Scaling factor
    target_modules=["c_attn", "c_proj"], # Attention matrices
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)
\end{lstlisting}

With $r=16$, each attention matrix gets two small $r \times d$ matrices. The \texttt{lora\_alpha} parameter scales the contribution of these matrices. I chose $\alpha = 32$ (twice $r$) to give the adaptations sufficient influence.

\subsection{Training Configuration}

Both models trained for 3 epochs on the full dataset. Here's why the hyperparameters matter:

\textbf{Learning Rate:} mBART used $5 \times 10^{-5}$ (conservative, since it's pre-trained) while GPT-2 used $2 \times 10^{-4}$ (higher, because LoRA matrices start random).

\textbf{Batch Size:} Limited by GPU memory—8 for mBART, 4 for GPT-2. Smaller batches for GPT-2 because the prompt format makes sequences longer.

\textbf{Warmup Steps:} 500 steps gradually increase the learning rate from 0. This prevents early training instability that can occur when adapting pre-trained models.

\textbf{Mixed Precision (FP16):} Reduces memory usage and speeds training. Modern GPUs have specialized hardware for 16-bit operations.

\begin{lstlisting}
training_args_mbart = Seq2SeqTrainingArguments(
    output_dir="./mbart_finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    learning_rate=5e-5,
    warmup_steps=500,
    weight_decay=0.01,
    fp16=True,
    logging_steps=100,
    save_steps=1000,
    predict_with_generate=True
)
\end{lstlisting}

\subsection{Evaluation Metrics}

Translation quality is multifaceted, so we need multiple metrics:

\subsubsection{BLEU (Bilingual Evaluation Understudy)}

BLEU measures n-gram overlap between translation and reference. It computes precision for 1-grams through 4-grams, then combines them:

$$\text{BLEU} = BP \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)$$

where $p_n$ is n-gram precision and $BP$ is a brevity penalty for short translations. BLEU is harsh—it requires exact matches, so even good translations can score low if they use synonyms.

\subsubsection{chrF (Character n-gram F-score)}

Instead of words, chrF looks at character sequences. This is particularly valuable for morphologically rich languages and catches partial matches:

$$\text{chrF} = \frac{2 \cdot \text{chrP} \cdot \text{chrR}}{\text{chrP} + \text{chrR}}$$

where chrP and chrR are character-level precision and recall. A score of 44.00 means the model got 44\% of character sequences right—much more forgiving than BLEU.

\subsubsection{TER (Translation Edit Rate)}

TER counts the minimum number of edits (insertions, deletions, substitutions, shifts) needed to change the translation into the reference:

$$\text{TER} = \frac{\text{Number of Edits}}{\text{Average Reference Length}} \times 100$$

Lower is better. TER of 66 means you need edits equal to 66\% of the reference length—substantial but not catastrophic.

\subsection{Test Datasets}

\textbf{Software Domain:} Specialized dataset of software documentation translations. Technical vocabulary, specific terminology, structured text. This tests whether models can handle domain-specific language.

\textbf{FLORES-200:} General domain dataset covering news, literature, conversations. I used 200 examples to get a sense of general translation ability.

\section{Results and Analysis}

\subsection{Quantitative Results}

\begin{table}[H]
\centering
\caption{Translation Quality Metrics}
\begin{tabular}{llccc}
\toprule
\textbf{Model} & \textbf{Dataset} & \textbf{BLEU} & \textbf{chrF} & \textbf{TER} \\
\midrule
mBART-50 & Software Domain & 19.26 & 44.00 & 66.19 \\
GPT-2+LoRA & Software Domain & 0.57 & 11.59 & 304.70 \\
\midrule
mBART-50 & General (FLORES-200) & 18.55 & 50.08 & 65.43 \\
GPT-2+LoRA & General (FLORES-200) & 1.50 & 16.90 & 210.69 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{What These Numbers Actually Mean}

The results are striking, and they tell a clear story about architectural advantages.

\subsubsection{mBART-50 Performance}

BLEU scores around 19 might seem low if you're used to seeing reported scores of 30-40 in research papers. But context matters. We're working with a relatively small training set (50K pairs) and evaluating on challenging specialized text. For reference, professional human translation typically scores 40-60 BLEU when compared to other human translations.

The chrF scores of 44-50 are more encouraging. Remember, chrF is character-based, so it catches partial matches. A score of 44 on software documentation means the model is getting vocabulary right—it might say "applicatie" instead of "toepassing" (both mean "application"), but the core meaning comes through.

TER around 65-66 indicates moderate edit distance. You'd need to change about two-thirds of the words to match the reference exactly. This sounds bad, but much of this is acceptable variation—different word order, synonym choices, or stylistic differences that don't change meaning.

\subsubsection{GPT-2+LoRA Performance}

The GPT-2 results are... disappointing. BLEU scores below 2 indicate the model is barely producing coherent translations. The TER scores over 200 are particularly telling—you'd need to make more edits than the reference is long, meaning the output is often longer and mostly wrong.

What happened? GPT-2 is trying to learn translation through a prompt-completion paradigm, but it's fundamentally designed for text continuation, not transformation. Even with 50,000 examples, it struggles to override its pre-trained behavior. When you give it "Translate to Dutch: Hello", it might continue with "world" instead of translating.

The chrF scores (11.59 and 16.90) being higher than BLEU suggests GPT-2 is getting individual characters and short sequences right. It's producing Dutch-like text, but not coherent Dutch translations.

\subsection{Qualitative Analysis}

Let me share what typical outputs looked like:

\textbf{Example 1 - Software Documentation:}
\begin{itemize}
    \item \textbf{English:} "Click the submit button to proceed with the installation."
    \item \textbf{Reference:} "Klik op de verzendknop om door te gaan met de installatie."
    \item \textbf{mBART:} "Klik op de verzendknop om verder te gaan met de installatie."
    \item \textbf{GPT-2:} "Klik de submit knop naar proceed met de installation process."
\end{itemize}

mBART's translation is nearly perfect—"verder te gaan" instead of "door te gaan" is a minor variation. GPT-2 code-switches, mixing English words ("submit", "proceed") with Dutch structure. It's trying, but failing.

\textbf{Example 2 - General Text:}
\begin{itemize}
    \item \textbf{English:} "The weather forecast predicts rain throughout the weekend."
    \item \textbf{Reference:} "De weersvoorspelling voorspelt regen gedurende het weekend."
    \item \textbf{mBART:} "De weersvoorspelling voorspelt regen tijdens het weekend."
    \item \textbf{GPT-2:} "De weer voorspel predicts regen throughout de weekend dagen."
\end{itemize}

Again, mBART produces natural Dutch with minor variation ("tijdens" vs "gedurende"). GPT-2 produces a Dutch-English hybrid that no native speaker would use.

\subsection{Domain Comparison}

Interestingly, both models perform slightly better on general text than software documentation:

\begin{itemize}
    \item mBART: General domain chrF (50.08) > Software domain chrF (44.00)
    \item GPT-2: General domain chrF (16.90) > Software domain chrF (11.59)
\end{itemize}

This makes sense. General text was better represented in both models' pre-training data. Software documentation contains specialized terms, UI element names, and technical jargon that appear less frequently in general corpora.

\subsection{Why mBART Wins}

The performance gap isn't close—mBART dramatically outperforms GPT-2. Let's understand why:

\textbf{Architectural Alignment:} mBART's encoder-decoder design mirrors the translation task. The encoder builds a language-agnostic representation, and the decoder generates target language. This separation is powerful.

\textbf{Bidirectional Encoding:} mBART's encoder sees the entire input at once. Each word's representation incorporates context from both left and right. GPT-2 only sees left context—it's always predicting the next word.

\textbf{Cross-Attention:} mBART's decoder attends back to the source sentence at each generation step. It can "look at" different parts of the input for different parts of the output. GPT-2 has no such mechanism—it must encode everything in the prompt's representation.

\textbf{Pre-training Objective:} mBART was pre-trained on denoising and translation-like tasks across 50 languages. GPT-2 was pre-trained only on English text completion. The prior knowledge matters.

\section{Code Implementation Details}

\subsection{Data Loading Pipeline}

The data loading function handles multiple sources gracefully:

\begin{lstlisting}
def load_production_corpus(max_samples=50000):
    datasets_loaded = []
    
    # Try each source, fail gracefully
    for source in [opus100, opus_books, tatoeba, ccmatrix]:
        try:
            ds = load_source(source)
            datasets_loaded.append(ds)
        except Exception as e:
            print(f"Skipping {source}: {e}")
    
    # Combine and filter
    combined = concatenate_datasets(datasets_loaded)
    combined = combined.filter(is_valid)
    combined = combined.shuffle(seed=42)
    
    if len(combined) > max_samples:
        combined = combined.select(range(max_samples))
    
    return combined
\end{lstlisting}

The seed=42 ensures reproducibility—same shuffle every time. This matters for comparing experiments.

\subsection{Inference Implementation}

The generation process differs significantly:

\begin{lstlisting}
# mBART generation
inputs = tokenizer_mbart(text, return_tensors="pt").to("cuda")
outputs = model_mbart.generate(
    **inputs,
    max_length=128,
    num_beams=5,        # Beam search
    early_stopping=True
)
translation = tokenizer_mbart.decode(outputs[0])

# GPT-2 generation
prompt = f"Translate to Dutch: {text}\n\nTranslation:"
inputs = tokenizer_gpt2(prompt, return_tensors="pt").to("cuda")
outputs = model_gpt2.generate(
    **inputs,
    max_new_tokens=128,  # Only generate new tokens
    num_beams=5,
    pad_token_id=tokenizer_gpt2.pad_token_id
)
full_text = tokenizer_gpt2.decode(outputs[0])
translation = full_text.split("Translation:")[-1].strip()
\end{lstlisting}

Beam search with 5 beams explores multiple hypotheses in parallel, keeping the top 5 at each step. This improves quality over greedy decoding.

\section{Challenges and Limitations}

\subsection{Computational Constraints}

Training large models is expensive. I used mixed precision training and gradient accumulation to fit within a single GPU, but this meant:
\begin{itemize}
    \item Smaller batch sizes than ideal
    \item Longer training time
    \item Limited hyperparameter search
\end{itemize}

\subsection{Dataset Size}

50,000 sentence pairs is respectable but not huge by modern standards. State-of-the-art systems train on millions of pairs. With more data, both models would improve, though I suspect the relative gap would persist.

\subsection{Evaluation Limitations}

Automatic metrics don't capture everything. A translation can be perfectly correct but score poorly because it uses different phrasing than the reference. Human evaluation would provide additional insight, but that's beyond the scope of this project.

\subsection{GPT-2 Prompt Engineering}

I used a simple prompt format. More sophisticated prompting (few-shot examples, chain-of-thought, etc.) might improve GPT-2's performance. However, the fundamental architectural limitations would remain.

\section{Lessons Learned}

\subsection{Architecture Matters}

You can't always overcome architectural constraints with data or training. Translation is a task with structure—source and target, alignment, reordering—and models designed for that structure have inherent advantages.

\subsection{Pre-training Direction}

The type of pre-training matters as much as the amount. mBART's multilingual, translation-oriented pre-training gave it a huge head start. GPT-2's monolingual, completion-oriented pre-training was less transferable.

\subsection{Parameter Efficiency Isn't Everything}

LoRA dramatically reduced trainable parameters (from ~124M to ~2M for GPT-2), but this didn't make the model competitive. Efficiency is valuable, but not at the cost of capability.

\subsection{Metrics Tell Stories}

Looking at multiple metrics revealed different facets of failure. GPT-2's higher-than-expected chrF scores showed it was learning some character-level patterns even though BLEU showed it wasn't producing good translations. This granular view is valuable.

\section{Future Directions}

\subsection{Model Improvements}

\textbf{Larger Models:} Testing with GPT-3-scale decoder-only models or mBART-large would show whether scale changes the conclusions.

\textbf{Better Prompting:} In-context learning with few-shot examples might help GPT-2-style models. Recent work on prompt optimization could be applied.

\textbf{Hybrid Approaches:} What if we added an encoder to GPT-2? Or taught mBART to do in-context learning?

\subsection{Evaluation Extensions}

\textbf{Human Evaluation:} Have native Dutch speakers rate fluency and adequacy.

\textbf{Error Analysis:} Categorize errors (word choice, grammar, word order) to understand failure modes.

\textbf{More Domains:} Test on legal, medical, literary text to understand generalization.

\subsection{Practical Applications}

\textbf{Deployment:} Build a web interface where users can compare translations from both models.

\textbf{Domain Adaptation:} Fine-tune further on specific domains (medical, legal) to see specialized performance.

\textbf{Interactive Systems:} Allow users to provide feedback and see how models improve.

\section{Conclusion}

This project set out to compare two fundamentally different approaches to neural machine translation: encoder-decoder (mBART-50) and decoder-only (GPT-2+LoRA) architectures. The results decisively favor the specialized encoder-decoder approach, which achieved BLEU scores of 19.26 (software) and 18.55 (general) compared to GPT-2's 0.57 and 1.50.

But the deeper insight isn't just "mBART is better"—it's that task-specific architectures have enduring value. As we build more general-purpose AI systems, we might assume that sufficient scale and data can overcome any architectural mismatch. This experiment suggests otherwise. Translation benefits from explicit source-target separation, bidirectional encoding, and cross-attention—features absent in pure decoder-only models.

Does this mean decoder-only models can't do translation? Not necessarily. Recent large language models (GPT-4, PaLM) handle translation well, suggesting that extreme scale might eventually overcome architectural limitations. But for practical applications with limited compute budgets, choosing the right architecture remains crucial.

The English-to-Dutch translation task, while specific, represents a broader class of sequence-to-sequence problems. The lessons here apply to summarization, paraphrasing, question answering, and other transformations where input and output have clear structure.

\section*{Acknowledgments}

This project used the Hugging Face Transformers library, pre-trained models from Facebook AI Research and OpenAI, and datasets from the OPUS project and Facebook AI. The computational resources were provided by Google Colab.

\begin{thebibliography}{9}

\bibitem{vaswani2017}
Vaswani, A., et al. (2017). Attention is all you need. \textit{Advances in Neural Information Processing Systems}, 30.

\bibitem{liu2020}
Liu, Y., et al. (2020). Multilingual denoising pre-training for neural machine translation. \textit{Transactions of the Association for Computational Linguistics}, 8, 726-742.

\bibitem{radford2019}
Radford, A., et al. (2019). Language models are unsupervised multitask learners. \textit{OpenAI blog}, 1(8), 9.

\bibitem{hu2021}
Hu, E. J., et al. (2021). LoRA: Low-rank adaptation of large language models. \textit{arXiv preprint arXiv:2106.09685}.

\bibitem{papineni2002}
Papineni, K., et al. (2002). BLEU: a method for automatic evaluation of machine translation. \textit{Proceedings of ACL}, 311-318.

\bibitem{post2018}
Post, M. (2018). A call for clarity in reporting BLEU scores. \textit{Proceedings of WMT}, 186-191.

\bibitem{popovic2015}
Popović, M. (2015). chrF: character n-gram F-score for automatic MT evaluation. \textit{Proceedings of WMT}, 392-395.

\bibitem{tiedemann2012}
Tiedemann, J. (2012). Parallel data, tools and interfaces in OPUS. \textit{Proceedings of LREC}, 2214-2218.

\bibitem{goyal2022}
Goyal, N., et al. (2022). The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. \textit{Transactions of ACL}, 10, 522-538.

\end{thebibliography}

\end{document}
